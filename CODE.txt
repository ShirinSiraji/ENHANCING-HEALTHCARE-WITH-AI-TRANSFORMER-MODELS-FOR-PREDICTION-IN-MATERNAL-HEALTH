# STEP-1 DATA PREPROCESSING
#Importing libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns
# Loading dataset data = pd.read_csv("/content/filtered ds_sdp.csv") data
#Size of the dataset data.size
741312

#Shape of the dataset data.shape
(30888, 24)
#Columns of the dataset data.columns
Index(['age', 'marital_status', 'religion', 'social_group_code', 'sex', 'delivered_any_baby', 'born_alive_total', 'surviving_total', 'mother_age_when_baby_was_born', 'outcome_pregnancy', 'is_currently_pregnant', 'pregnant_month', 'is_anc_registered', 'is_any_fp_methos_used', 'fp_method_used', 'reason_for_not_using_fp_method', 'aware_abt_rti', 'aware_abt_hiv', 'aware_of_haf', 'aware_of_the_danger_signs', 'currently_attending_school', 'reason_for_not_attending_school', 'usual_residance', 'relation_to_head'], dtype='object')
# Dataset description data.describe()
#Checking missing values missing = data.isnull().sum() missing = missing[missing > 0] print(missing)
religion 3 social_group_code 3 sex 3 delivered_any_baby 3943 born_alive_total 5946 surviving_total 5946 mother_age_when_baby_was_born 5950 outcome_pregnancy 3943 is_currently_pregnant 4606 pregnant_month 29296 is_anc_registered 29296 is_any_fp_methos_used 10250

fp_method_used 16015 reason_for_not_using_fp_method 25215 aware_abt_rti 4517 aware_abt_hiv 4517 aware_of_haf 22074 aware_of_the_danger_signs 4517 currently_attending_school 30725 reason_for_not_attending_school 30765 usual_residance 4095 relation_to_head 3 dtype: int64
#STEP-2 FEATURE ENGINEERING
#Droping columns with too many missing values columns_to_drop = ['pregnant_month', 'is_anc_registered', 'aware_of_haf', 'is_any_fp_methos_used', 'fp_method_used', 'reason_for_not_using_fp_method', 'currently_attending_school', 'reason_for_not_attending_school'] data.drop(columns=columns_to_drop, inplace=True)
#Filling missing values #Filling with most frequent value for cols with low missing values (mode) for col in ['religion', 'social_group_code', 'sex', relation_to_head']: data[col].fillna(data[col].mode()[0], inplace=True)
# Filling with "Unknown" for categorical columns for col in ['delivered_any_baby', 'outcome_pregnancy', 'is_currently_pregnant', 'usual_residance']: data[col].fillna('Unknown', inplace=True)
# Filling with 0 for numerical baby-related columns (no baby delivered => 0 babies) for col in ['born_alive_total', 'surviving_total', 'mother_age_when_baby_was_born']: data[col].fillna(0, inplace=True)
# Filling with "Not aware" for awareness columns for col in ['aware_abt_rti', 'aware_abt_hiv', 'aware_of_the_danger_signs']: data[col].fillna('Not aware', inplace=True)
#Saving the cleaned dataset data.to_csv('cleaned_filtered_ds_sdp.csv', index=False) print("Data cleaned and saved successfully!")

Data cleaned and saved successfully!
#New dataset data1 data1 = pd.read_csv("/content/cleaned_filtered_ds_sdp.csv") data1
#Label Encoding for categorical to numerical values
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
# List of columns to encode columns_to_encode = ['religion', 'social_group_code', 'sex', 'delivered_any_baby', 'outcome_pregnancy', ‘is_currently_pregnant', 'usual_residance', 'relation_to_head', 'aware_abt_rti', 'aware_abt_hiv', 'aware_of_the_danger_signs','marital_status']
# Applying Label Encoding for col in columns_to_encode: data[col] = le.fit_transform(data[col]) # Save the fully encoded dataset data.to_csv('final_dataset_encoded.csv', index=False) print("Final dataset saved as 'final_dataset_encoded.csv'.")
Final dataset saved as 'final_dataset_encoded.csv'.

#New dataset after encoding data2 = pd.read_csv("/content/final_dataset_encoded.csv") data2
#STEP-3 DATA VISUALIZATION
#Plot: Histogram + KDE for age and mother_age_when_baby_was_born #To understand age spread and skewness sns.histplot(data=data, x='age', kde=True) plt.title("Distribution of Age")
Text(0.5, 1.0, 'Distribution of Age')

# Bar Plot of Target Variable sns.countplot(x='outcome_pregnancy', data=data)
<Axes: xlabel='outcome_pregnancy', ylabel='count'>
#Box plot to spread of the variable 'delivered_any_baby' wrt to age sns.boxplot(x='delivered_any_baby', y='age', data=data)
<Axes: xlabel='delivered_any_baby', ylabel='age'>

#Heatmap: To detect multicollinearity and relationship between the variables. plt.figure(figsize=(12,8)) sns.heatmap(data.corr(), annot=True, cmap='coolwarm') plt.title("Correlation Heatmap")
Text(0.5, 1.0, 'Correlation Heatmap')
#Boxplot to understand the target variable classes spread with respect to age. sns.boxplot(x='outcome_pregnancy', y='age', data=data)
<Axes: xlabel='outcome_pregnancy', ylabel='age'>

#STEP-4 TRAIN TEST SPLIT OF THE DATA
#Separating Features and Target
from sklearn.model_selection import train_test_split X = data2.drop('outcome_pregnancy', axis=1) y = data2['outcome_pregnancy']
#Spliting into Train and Test sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y) print("Training size:", X_train.shape) print("Testing size:", X_test.shape)
Training size: (24710, 15) Testing size: (6178, 15)
#STEP-5 MODEL BUILDING
##1.IMPLEMENTATION OF LOGISTIC REGRESSION
from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
#Creating Logistic Regression model log_reg = LogisticRegression(max_iter=1000) #Training the model log_reg.fit(X_train, y_train)
LogisticRegression(max_iter=1000)
#Predicting on Test data y_pred = log_reg.predict(X_test)
#Evaluating the model print("The Logistic Regression Model Results:") print("Accuracy :", accuracy_score(y_test, y_pred)) print("Precision:", precision_score(y_test, y_pred, average='weighted')) print("Recall :", recall_score(y_test, y_pred, average='weighted')) print("F1 Score :", f1_score(y_test, y_pred, average='weighted')) print("\nClassification Report:") print(classification_report(y_test, y_pred))
The Logistic Regression Model Results: Accuracy : 0.8458

Precision: 0.8312 Recall : 0.8458 F1 Score : 0.8325 Classification Report: precision recall f1-score support
0 0.86 0.91 0.88 4342
1 1.00 0.99 0.99 789
2 0.57 0.40 0.47 1047
accuracy 0.85 6178
macro avg 0.81 0.77 0.78 6178
weighted avg 0.83 0.85 0.83 6178
##2.IMPLEMENTATION OF RANDOM FOREST
#Creating Random Forest model
from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(n_estimators=100, random_state=42)
#Training the model rf.fit(X_train, y_train) #Predicting on Test data y_pred_rf = rf.predict(X_test)
#Evaluation print("The Random Forest Model Results:") print("Accuracy :", accuracy_score(y_test, y_pred_rf)) print("Precision:", precision_score(y_test, y_pred_rf, average='weighted')) print("Recall :", recall_score(y_test, y_pred_rf, average='weighted')) print("F1 Score :", f1_score(y_test, y_pred_rf, average='weighted'))
print("\nClassification Report:") print(classification_report(y_test, y_pred_rf))
The Random Forest Model Results: Accuracy : 0.8596 Precision: 0.8431 Recall : 0.8596

F1 Score : 0.8450
Classification Report: precision recall f1-score support
0 0.88 0.93 0.90 4342
1 1.00 1.00 1.00 789
2 0.60 0.42 0.49 1047
accuracy 0.86 6178
macro avg 0.83 0.78 0.80 6178
weighted avg 0.84 0.86 0.85 6178
#FEATURE IMPORTANT ANALYSIS
#Get feature importances
import pandas as pd import matplotlib.pyplot as plt importances = rf.feature_importances_ #Creating DataFrame feature_importance_df = pd.DataFrame({ 'Feature': X_train.columns, 'Importance': importances })
#Sort and plot feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False) # Plot plt.figure(figsize=(10,6)) plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance']) plt.gca().invert_yaxis() plt.title('Feature Importance from Random Forest') plt.xlabel('Importance') plt.show()
Feature Importance 0 age 2.265647e-01 5 delivered_any_baby 1.667382e-01 8 mother_age_when_baby_was_born 1.643424e-01 6 born_alive_total 1.186249e-01

7 surviving_total 9.204483e-02 12 aware_of_the_danger_signs 4.505817e-02 9 is_currently_pregnant 4.018675e-02 14 relation_to_head 3.335336e-02 3 social_group_code 2.938431e-02 10 aware_abt_rti 2.778902e-02 11 aware_abt_hiv 2.553439e-02 1 marital_status 1.261914e-02 13 usual_residance 1.218138e-02 2 religion 5.578569e-03 4 sex 2.527238e-08
#3.IMPLEMENTATION OF TAB TRANSFORMERS
import pandas as pd import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers
# Ensure reproducibility np.random.seed(42) tf.random.set_seed(42)
# Shape details n_features = X_train.shape[1] n_classes = len(np.unique(y_train))

# Input Layer inputs = keras.Input(shape=(n_features,), name='Input')
# Dense projection to simulate embedding x = layers.Dense(64, activation='relu')(inputs) # Wrap tf.expand_dims in a Lambda layer x_expanded = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(x) # Shape: (batch_size, 1, 64) # Transformer Encoder Block attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x_expanded, x_expanded) attention_output = layers.Flatten()(attention_output)
# Feed Forward Network x = layers.Dense(128, activation='relu')(x) x = layers.Dropout(0.3)(x) x = layers.Dense(64, activation='relu')(x) # Output Layer outputs = layers.Dense(n_classes, activation='softmax', name='Output')(x)
# Define and Compile Model transformer_model = keras.Model(inputs=inputs, outputs=outputs, name="Tabular_Transformer") transformer_model.compile( optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'] )
# Fit Model history = transformer_model.fit( X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=32, verbose=1 )
Epoch 1/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 6s 5ms/step - accuracy: 0.7881 - loss: 0.4850 - val_accuracy: 0.8608 - val_loss: 0.3127 Epoch 2/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 4s 5ms/step - accuracy: 0.8561 - loss: 0.3151 - val_accuracy: 0.8650 - val_loss: 0.3015 Epoch 3/30

773/773 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - accuracy: 0.8627 - loss: 0.3077 - val_accuracy: 0.8624 - val_loss: 0.2997 Epoch 4/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 5s 6ms/step - accuracy: 0.8628 - loss: 0.3033 - val_accuracy: 0.8661 - val_loss: 0.2965 Epoch 5/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 3s 4ms/step - accuracy: 0.8643 - loss: 0.2986 - val_accuracy: 0.8665 - val_loss: 0.2956 Epoch 6/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 3s 4ms/step - accuracy: 0.8668 - loss: 0.2979 - val_accuracy: 0.8669 - val_loss: 0.2943 Epoch 7/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 6s 5ms/step - accuracy: 0.8683 - loss: 0.2977 - val_accuracy: 0.8658 - val_loss: 0.2953 Epoch 8/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 3s 4ms/step - accuracy: 0.8674 - loss: 0.2976 - val_accuracy: 0.8621 - val_loss: 0.2952 Epoch 9/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 3s 4ms/step - accuracy: 0.8658 - loss: 0.2967 - val_accuracy: 0.8673 - val_loss: 0.2935 Epoch 10/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 7s 6ms/step - accuracy: 0.8685 - loss: 0.2944 - val_accuracy: 0.8661 - val_loss: 0.2948 Epoch 11/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - accuracy: 0.8690 - loss: 0.2932 - val_accuracy: 0.8503 - val_loss: 0.3500 Epoch 12/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - accuracy: 0.8668 - loss: 0.3042 - val_accuracy: 0.8655 - val_loss: 0.2936 Epoch 13/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 3s 3ms/step - accuracy: 0.8692 - loss: 0.2903 - val_accuracy: 0.8653 - val_loss: 0.2934 Epoch 14/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - accuracy: 0.8674 - loss: 0.2927 - val_accuracy: 0.8652 - val_loss: 0.2940 Epoch 15/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 4s 4ms/step - accuracy: 0.8687 - loss: 0.2920 - val_accuracy: 0.8657 - val_loss: 0.2923 Epoch 16/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - accuracy: 0.8675 - loss: 0.2936 - val_accuracy: 0.8640 - val_loss: 0.2916 Epoch 17/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - accuracy: 0.8706 - loss: 0.2903 - val_accuracy: 0.8644 - val_loss: 0.2907 Epoch 18/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 3s 3ms/step - accuracy: 0.8694 -

loss: 0.2905 - val_accuracy: 0.8658 - val_loss: 0.2928 Epoch 19/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 3s 4ms/step - accuracy: 0.8703 - loss: 0.2892 - val_accuracy: 0.8661 - val_loss: 0.2912 Epoch 20/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - accuracy: 0.8695 - loss: 0.2903 - val_accuracy: 0.8621 - val_loss: 0.2925 Epoch 21/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - accuracy: 0.8676 - loss: 0.2902 - val_accuracy: 0.8652 - val_loss: 0.2913 Epoch 22/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - accuracy: 0.8694 - loss: 0.2890 - val_accuracy: 0.8666 - val_loss: 0.2911 Epoch 23/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 3s 4ms/step - accuracy: 0.8683 - loss: 0.2890 - val_accuracy: 0.8631 - val_loss: 0.2918 Epoch 24/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - accuracy: 0.8691 - loss: 0.2881 - val_accuracy: 0.8634 - val_loss: 0.2934 Epoch 25/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 3s 3ms/step - accuracy: 0.8689 - loss: 0.2903 - val_accuracy: 0.8655 - val_loss: 0.2920 Epoch 26/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - accuracy: 0.8692 - loss: 0.2881 - val_accuracy: 0.8635 - val_loss: 0.2935 Epoch 27/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - accuracy: 0.8698 - loss: 0.2890 - val_accuracy: 0.8668 - val_loss: 0.2898 Epoch 28/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 3s 4ms/step - accuracy: 0.8696 - loss: 0.2870 - val_accuracy: 0.8668 - val_loss: 0.2898 Epoch 29/30 773/773 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - accuracy: 0.8701 - loss: 0.2875 - val_accuracy: 0.8669 - val_loss: 0.2904 Epoch 30/30
773/773 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - accuracy: 0.8711 - loss: 0.2881 - val_accuracy: 0.8687 - val_loss: 0.2900
#4.IMPLEMENTATION OF THE XAI-SHAP TECHNIQUE
import shap import matplotlib.pyplot as plt # Use KernelExplainer explainer = shap.KernelExplainer(transformer_model.predict, shap.sample(X_train, 100))

# Compute SHAP values shap_values = explainer.shap_values(X_test[:100])
4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step
{"model_id":"9a6c86274c8c47c0a3ed3581b51ae5d0","version_major":2,"version_minor":0}
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 37ms/step 6488/6488 ━━━━━━━━━━━━━━━━━━━━ 8s 1ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step
6488/6488 ━━━━━━━━━━━━━━━━━━━━ 8s 1ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 62ms/step
6488/6488 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step
6488/6488 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step
6488/6488 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 50ms/step
6488/6488 ━━━━━━━━━━━━━━━━━━━━ 8s 1ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 43ms/step
6488/6488 ━━━━━━━━━━━━━━━━━━━━ 8s 1ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step
6488/6488 ━━━━━━━━━━━━━━━━━━━━ 8s 1ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step
6488/6488 ━━━━━━━━━━━━━━━━━━━━ 8s 1ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step
6488/6488 ━━━━━━━━━━━━━━━━━━━━ 8s 1ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 43ms/step
6488/6488 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step
6488/6488 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step
………………………………
………………………………
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step 6488/6488 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step
# Assuming your model has 3 classes (based on the shap_values shape) num_classes = 3 # Iterate through each class and plot SHAP values separately
import shap import matplotlib.pyplot as plt

for class_index in range(num_classes): # Select SHAP values for the current class class_shap_values = shap_values[:, :, class_index]
# Plot summary plot for the current class shap.summary_plot(class_shap_values, X_test[:100], feature_names=X_train.columns, title=f"SHAP Values for Class {class_index}")
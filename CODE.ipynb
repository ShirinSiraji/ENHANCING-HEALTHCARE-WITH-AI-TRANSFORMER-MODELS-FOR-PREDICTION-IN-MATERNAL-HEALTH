{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555f46ea",
   "metadata": {},
   "source": [
    "# ENHANCING HEALTHCARE WITH AI: TRANSFORMER MODELS FOR PREDICTION IN MATERNAL HEALTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4dfa8e",
   "metadata": {},
   "source": [
    "\n",
    "## Project Overview\n",
    "I conducted this project to analyze maternal health data and identify key risk factors influencing pregnancy outcomes. The overall objective was to build reliable and interpretable predictive models that can support early risk identification and informed healthcare decision-making. The workflow includes data preprocessing, exploratory data analysis, model development, evaluation, and explainable AI techniques.\n",
    "\n",
    "Maternal health remains a critical concern in many rural regions of India, where limited access to timely medical care and inadequate healthcare infrastructure often lead to preventable complications during pregnancy. Through this project, I aimed to contribute toward addressing this challenge by developing a data-driven and interpretable predictive framework for maternal health risk assessment.\n",
    "\n",
    "I worked with a publicly available dataset from the Open Government Data Platform (India), consisting of approximately 30,000 records and 24 features such as age, blood pressure, blood sugar levels, and pregnancy history. Extensive preprocessing was performed, including handling missing values, feature engineering, and exploratory data analysis to ensure data quality and reliability.\n",
    "\n",
    "I developed and compared three predictive models: Logistic Regression, Random Forest, and a TabTransformer-based deep learning model. Each model was evaluated using appropriate performance metrics to assess predictive capability. Among them, the TabTransformer demonstrated strong performance by effectively capturing complex relationships between features.\n",
    "\n",
    "To enhance model interpretability and trustworthiness, I applied SHAP (SHapley Additive exPlanations) to understand feature contributions. The results highlighted that factors such as blood pressure, maternal age, and blood sugar levels were among the most influential predictorsâ€”aligning well with established clinical knowledge.\n",
    "\n",
    "By integrating explainable AI techniques with predictive modeling, this project demonstrates how data-driven approaches can support transparent and informed healthcare decision-making. The findings highlight the potential of interpretable deep learning models in improving maternal healthcare outcomes, particularly in resource-constrained and rural settings. Future work will focus on incorporating real-time monitoring, expanding the dataset, and enhancing model generalizability to improve real-world applicability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181b6de2",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "### In this step, we import all required libraries, load the dataset, and perform basic inspection to understand its structure, size, and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461acf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "data = pd.read_csv(\"/content/filtered ds_sdp.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fd7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Size\n",
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d25f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582cb7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical Summary\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f46c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Missing Values\n",
    "missing = data.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5b2fc",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering\n",
    "### This step focuses on handling missing values, removing unnecessary columns, and preparing the dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Columns with High Missing Values\n",
    "columns_to_drop = [\n",
    "    'pregnant_month', 'is_anc_registered', 'aware_of_haf',\n",
    "    'is_any_fp_methos_used', 'fp_method_used',\n",
    "    'reason_for_not_using_fp_method',\n",
    "    'currently_attending_school', 'reason_for_not_attending_school'\n",
    "]\n",
    "\n",
    "data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling Missing Values\n",
    "#Filling categorical values using mode:\n",
    "for col in ['religion', 'social_group_code', 'sex', 'relation_to_head']:\n",
    "    data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "\n",
    "#Filling with \"Unknown\" where appropriate:\n",
    "for col in ['delivered_any_baby', 'outcome_pregnancy', 'is_currently_pregnant', 'usual_residance']:\n",
    "    data[col].fillna('Unknown', inplace=True)\n",
    "\n",
    "#Filling numerical columns with zero:\n",
    "for col in ['born_alive_total', 'surviving_total', 'mother_age_when_baby_was_born']:\n",
    "    data[col].fillna(0, inplace=True)\n",
    "\n",
    "#Awareness columns:\n",
    "for col in ['aware_abt_rti', 'aware_abt_hiv', 'aware_of_the_danger_signs']:\n",
    "    data[col].fillna('Not aware', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a6d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Cleaned Dataset\n",
    "data.to_csv('cleaned_filtered_ds_sdp.csv', index=False)\n",
    "print(\"Data cleaned and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ce408",
   "metadata": {},
   "source": [
    "## Step 3: Encoding Categorical Variables\n",
    "### Machine learning models require numerical input. Here, categorical features are encoded using Label Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254efe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data1 = pd.read_csv(\"/content/cleaned_filtered_ds_sdp.csv\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "columns_to_encode = [\n",
    "    'religion', 'social_group_code', 'sex', 'delivered_any_baby',\n",
    "    'outcome_pregnancy', 'is_currently_pregnant',\n",
    "    'usual_residance', 'relation_to_head',\n",
    "    'aware_abt_rti', 'aware_abt_hiv',\n",
    "    'aware_of_the_danger_signs', 'marital_status'\n",
    "]\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    data1[col] = le.fit_transform(data1[col])\n",
    "\n",
    "# Load Encoded Dataset\n",
    "data1.to_csv('final_dataset_encoded.csv', index=False)\n",
    "print(\"Final dataset saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54945499",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of Age\n",
    "sns.histplot(data=data1, x='age', kde=True)\n",
    "plt.title(\"Distribution of Age\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a579a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Variable Distribution\n",
    "sns.countplot(x='outcome_pregnancy', data=data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot Analysis\n",
    "sns.boxplot(x='delivered_any_baby', y='age', data=data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0af25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(data1.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff26eab6",
   "metadata": {},
   "source": [
    "## Step 5: Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating Features and Target\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data1.drop('outcome_pregnancy', axis=1)\n",
    "y = data1['outcome_pregnancy']\n",
    "\n",
    "#Spliting into Train and Test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training size:\", X_train.shape)\n",
    "print(\"Testing size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef72e01",
   "metadata": {},
   "source": [
    "# Step 6: MODEL BUILDING\n",
    "##  Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "#Creating Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "#Training the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "LogisticRegression(max_iter=1000)\n",
    "\n",
    "#Predicting on Test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08900ba1",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce66963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "#Training the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#Predicting on Test data\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "#Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_rf, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_rf, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_rf, average='weighted'))\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c77a9",
   "metadata": {},
   "source": [
    "## Step 7: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e48cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "#Creating DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d6474e",
   "metadata": {},
   "source": [
    "## Step 8: Tabular Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b01570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Shape details\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "# Input Layer\n",
    "inputs = keras.Input(shape=(n_features,), name='Input')\n",
    "\n",
    "# Dense projection to simulate embedding\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(x)\n",
    "\n",
    "# Transformer Encoder Block\n",
    "attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
    "attention_output = layers.Flatten()(attention_output)\n",
    "\n",
    "# Feed Forward Network\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "outputs = layers.Dense(n_classes, activation='softmax', name='Output')(x)\n",
    "\n",
    "# Define and Compile Model\n",
    "transformer_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "transformer_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fit Model\n",
    "history = transformer_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a88bf",
   "metadata": {},
   "source": [
    "## Step 10: Explainable AI (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de862dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Use KernelExplainer\n",
    "explainer = shap.KernelExplainer(transformer_model.predict, shap.sample(X_train, 100))\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values(X_test[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f863bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your model has 3 classes (based on the shap_values shape)\n",
    "num_classes = 3\n",
    "\n",
    "# Iterate through each class and plot SHAP values separately\n",
    "for class_index in range(num_classes):\n",
    "    class_shap_values = shap_values[:, :, class_index]\n",
    "    \n",
    "    # Plot summary plot for the current class\n",
    "    shap.summary_plot(\n",
    "        class_shap_values,\n",
    "        X_test[:100],\n",
    "        feature_names=X_train.columns,\n",
    "        title=f\"SHAP Values for Class {class_index}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
